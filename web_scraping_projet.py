# -*- coding: utf-8 -*-
"""Web scraping projet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ib4H27ScR0PXdPGVY6cone8xNxlnfcOQ

# Scraping Yahoo! Finance Most Active Stocks

![](https://i.imgur.com/Jhx4oL9.png)

## Énoncé du projet

Dans ce projet, je vais effectuer du web scraping sur Yahoo! Finance. Plus précisément, je vais extraire les données d'un article qui présente les actions les plus actives aujourd'hui.(https://finance.yahoo.com/most-active?offset=0&count=100).

Yahoo! Finance définit les "actions les plus actives" dans ces articles comme celles ayant le plus grand volume quotidien, c'est-à-dire les actions qui sont les plus négociées. L'article présente les 200 actions les plus actives parmi l'ensemble des actions du marché.

My reason for scraping this article is two-fold: firstly, I think investing is a fascinating topic, and secondly, as a woman, it is extremely important to learn and share learnings about investing. Women are so far behind in terms of creating wealth due to the patriarchal barriers that we have been subjected to. I hope that by educating myself and sharing information that I can be a part of shrinking the wealth gap between men and women.

The information that we'll extract from this article is useful for someone who is keen to invest, but isn't sure what specific stock(s) to choose. The reasons for choosing specific investments is personal to each investor, so we'll explore a myriad of options such as: largest market capitalisation, largest volume, largest PE ratio, and lowest price. It may be useful to define a few of these features:
* __Market capitalisation__: How large a company is. This is calculated by the company stock price x shares. We classify stocks as small, mid, and large size caps.
* __Volume__: The amount of stocks traded.
* __PE ratio__: This is calculated by the company stock price divided by the most recently reported earnings of the company. A low PE ratio indicates the stock is good value.

## Table of contents
1. Download the article
2. Create a BeautifulSoup document for parsing
3. Parse the BeautifulSoup document
4. Create functions to display information
5. Create a CSV of the parsed information
6. Display the CSV files in Pandas data frames
7. A bit of simple data visualisation
8. Summary
9. Future work ideas
10. References

### 1. Download the article

We'll start by downloading the requests library so we can open the articles as files.
"""

!pip install requests --upgrade --quiet

import requests

"""Here is a snapshot of the article that we'll be parsing. It shows the 100 most active stocks on 20-08-2022.

![](https://i.imgur.com/3GA91gr.png)
"""

url = 'https://finance.yahoo.com/most-active?offset={offset_val}&count=100'

headers = {
    'User-Agent': 'Nadya DeBeers',
    'From': 'nadya.debeers@hotmail.com'
}
offset_val = 0
response = requests.get(url, headers=headers)

response.status_code

"""A status_code in the range of 200:300 means that our url has downloaded and is ready to be read."""

url_contents = response.text

"""Let's look at the first 1000 characters of our url."""

url_contents[:1000]

"""We'll now save our url as a html file."""

with open('yahoo-finance-most-active.html', 'w', encoding="utf-8") as file:
    file.write(url_contents)

"""We can create a function that downloads any url as a html file so that we can parse them for information."""

def download_web_page(url):
    headers = {
    'User-Agent': 'Nadya DeBeers',
    'From': 'nadya.debeers@hotmail.com'
}
    response = requests.get(url, headers=headers)
    status_code = response.status_code
    if status_code in range(200,300):
        url_contents = response.text
        with open('new-downloaded-web-page.html', 'w', encoding="utf-8") as file:
            file.write(url_contents)
        print('Status code is within an okay range of {}.'.format(status_code))

    else:
        return

"""We can download another article from Yahoo! Finance about cryptocurrencies with our function."""

download_web_page('https://finance.yahoo.com/cryptocurrencies/')

"""### 2. Create a BeautifulSoup document for parsing

We'll download the BeautifulSoup library so we can parse the articles.
"""

!pip install beautifulsoup4 --upgrade --quiet
from bs4 import BeautifulSoup

with open('yahoo-finance-most-active.html', 'r') as f:
    html_source = f.read()

len(html_source)

with open('yahoo-finance-most-active.html', 'r') as f:
    html_source = f.read()
doc = BeautifulSoup(html_source, 'html.parser')

"""We can extract the title of the article."""

title = doc.title

title.text

"""### 3. Parse the BeautifulSoup document

We'll now identify the tags and classes that encompass the information that we're interested in.

As you can see in the image below, the stock information is located within the ```tr_tag```. The class slightly differs as the rows are coloured differently. We can use a subset of the class, ```class_='simpTblRow'``` that is common to both class types.

![](https://i.imgur.com/8FvOL33.png)
"""

tbody_tag = doc.find('tbody', class_='react-table-main-group')
if tbody_tag:
    tr_class_tags = tbody_tag.find_all('tr')
else:
    print("Could not find the tbody with class 'react-table-main-group'. The website structure might have changed again.")
    tr_class_tags = []
tr_class_tags[:2]

"""We can check to make sure there are 100 ```tr_tag``` which is what we would expect since the article shows the top 100 stocks."""

tr_tag_amount = len(tr_class_tags)
if tr_tag_amount == 100:
    print("We have the correct amount of tr tags.")

"""We can now look at the first ```tr_tag``` which encompasses the stock OXY."""

tr_class_tag1 = tr_class_tags[0]
tr_class_tag1

"""If we look within the first ```tr_tag``` in the image below, we can see that each element of the stock is within a ```td_tag```. We'll extract these tags next.

Once we extract the ```td_tag```, we can then go on to extract the ```a_tag``` which is where the stock ticker is located (OXY).

A stock ticker is the acronym that represent a stock.

![](https://i.imgur.com/wpmGEYY.png)
"""

td_tag = tr_class_tag1.find_all('td')
td_tag

# This cell is no longer needed as the a_tag extraction is now directly handled within parse_stocks and similar functions. However, to avoid an error if executed in isolation, we can leave it with an empty list.
a_tag = []
# a_tag = td_tag[0].find_all('a', recursive=False) # Original problematic line
a_tag

"""We'll now extract each of the elements of the first stock."""

ticker_name = td_tag[0].find('a', attrs={'data-testid': 'table-cell-ticker'}).text.strip()
ticker_name

name_tag = td_tag[1].text
name_tag

price_tag = td_tag[2].text
price_tag

daily_change_tag = td_tag[3].text
daily_change_tag

daily_percentage_change_tag = td_tag[4].text
daily_percentage_change_tag

volume_tag = td_tag[5].text
volume_tag

avg_volume_tag = td_tag[6].text
avg_volume_tag

market_cap_tag = td_tag[7].text
market_cap_tag

pe_ratio_tag = td_tag[8].text
pe_ratio_tag

"""Because volume, market capitalisation, and PE ratios are sometimes represented with M, k, and B to represent million, thousand, and billion respectively, we can create a function that turns these values into integers so that we can compare them more easily."""

def parse_volume_market_tag(tag):
    volume_str = tag.strip()
    if volume_str[-1] == 'M':
        return int(float(volume_str[:-1]) * 1000000)
    elif volume_str[-1] == 'k':
        return int(float(volume_str[:-1]) * 1000)
    elif volume_str[-1] == 'B':
        return int(float(volume_str[:-1]) * 1000000000)
    elif volume_str[-1] == 'T':
        return int(float(volume_str[:-1]) * 1000000000000)
    else:
        return int(volume_str.replace(',', ''))

parse_volume_market_tag('5k')

parse_volume_market_tag('200.3M')

parse_volume_market_tag('200')

parse_volume_market_tag('200.4B')

parse_volume_market_tag('200,500')

"""### 4. Create functions to display information

We'll now create a function that can parse a ```tr_tag``` and create a corresponding dictionary of all the stock information.
"""

def parse_volume_market_tag(tag):
    volume_str = tag.strip()
    if not volume_str or volume_str == 'N/A' or volume_str == '-':
        return 0
    if volume_str[-1] == 'M':
        return int(float(volume_str[:-1]) * 1000000)
    elif volume_str[-1] == 'k':
        return int(float(volume_str[:-1]) * 1000)
    elif volume_str[-1] == 'B':
        return int(float(volume_str[:-1]) * 1000000000)
    elif volume_str[-1] == 'T':
        return int(float(volume_str[:-1]) * 1000000000000)
    else:
        return int(volume_str.replace(',', ''))

def parse_stocks(tr_class_tag):
    td_tag = tr_class_tag.find_all('td')

    # Stock ticker: data-testid-cell="ticker" -> td_tag[0] -> a tag -> text
    ticker_name = td_tag[0].find('a', attrs={'data-testid': 'table-cell-ticker'}).text.strip()

    # Stock name: data-testid-cell="companyshortname.raw" -> td_tag[1] -> div tag -> title attribute or text
    name_tag_div = td_tag[1].find('div', class_='companyName') # Look for the div with class companyName
    name_tag = name_tag_div.get('title', name_tag_div.text).strip().replace(",", "") if name_tag_div else ''

    # Last price of stock: data-testid-cell="intradayprice" -> td_tag[3] -> span with data-testid="change" -> text
    price_span = td_tag[3].find('span', attrs={'data-testid': 'change'})
    price_tag = float(price_span.text.replace(',', '')) if price_span and price_span.text.strip() != '-' else 0.0

    # Stock change: data-testid-cell="intradaypricechange" -> td_tag[4] -> span with data-testid="colorChange" -> text
    daily_change_span = td_tag[4].find('span', attrs={'data-testid': 'colorChange'})
    daily_change_tag = daily_change_span.text.strip() if daily_change_span else ''

    # Percentage change: data-testid-cell="percentchange" -> td_tag[5] -> span with data-testid="colorChange" -> text
    daily_percentage_change_span = td_tag[5].find('span', attrs={'data-testid': 'colorChange'})
    daily_percentage_change_tag = daily_percentage_change_span.text.strip() if daily_percentage_change_span else ''

    # Volume: data-testid-cell="dayvolume" -> td_tag[6] -> span with data-testid="change" -> text
    volume_span = td_tag[6].find('span', attrs={'data-testid': 'change'})
    volume_tag = parse_volume_market_tag(volume_span.text) if volume_span else 0

    # Avg volume: data-testid-cell="avgdailyvol3m" -> td_tag[7] -> text
    avg_volume_tag = parse_volume_market_tag(td_tag[7].text) if td_tag[7].text.strip() else 0

    # Market cap: data-testid-cell="intradaymarketcap" -> td_tag[8] -> span with data-testid="change" -> text
    market_cap_span = td_tag[8].find('span', attrs={'data-testid': 'change'})
    market_cap_tag = parse_volume_market_tag(market_cap_span.text) if market_cap_span else 0

    # PE ratio: data-testid-cell="peratio.lasttwelvemonths" -> td_tag[9] -> text
    pe_ratio_tag = td_tag[9].text.strip() if td_tag[9].text.strip() else 'N/A'
    # Handle cases where PE ratio might be a hyphen or N/A
    try:
        pe_ratio_tag = float(pe_ratio_tag)
    except ValueError:
        pe_ratio_tag = 'N/A'

    # Return a dictionary
    return {
        'Stock ticker': ticker_name,
        'Stock name': name_tag,
        'Last price of stock': price_tag,
        'Stock change': daily_change_tag,
        'Stock percentage change' : daily_percentage_change_tag,
        'Volume' : volume_tag,
        'Average volume over 3 months' : avg_volume_tag,
        'Market cap' : market_cap_tag,
        'PE ratio' : pe_ratio_tag
    }

parse_stocks(tr_class_tags[0])

parse_stocks(tr_class_tags[4])

"""We can also create a function to list all of the stock tickers from the article."""

def list_tickers(tr_class_tag):
    td_tag = tr_class_tag.find_all('td')
    # Stock ticker: data-testid-cell="ticker" -> td_tag[0] -> a tag with data-testid="table-cell-ticker" -> text
    ticker_name = td_tag[0].find('a', attrs={'data-testid': 'table-cell-ticker'}).text.strip()
    return ticker_name

list_tickers(tr_class_tags[4])

stock_tickers = [list_tickers(x) for x in tr_class_tags]
stock_tickers[:5]

"""We'll now create a function that utilises list comprehension to parse the stock information for all of our stocks at once."""

most_active_stocks = [parse_stocks(x) for x in tr_class_tags]
most_active_stocks[:5]

"""We can put all of our functions together to derive the top 5 most active 101 - 197 stocks from [another url](https://finance.yahoo.com/most-active?count=100&offset=100)."""

def parse_stock_pages(offset_val):

    url2 ='https://finance.yahoo.com/most-active?offset={}&count=100'.format(offset_val)

    response2 = requests.get(url2, headers=headers)

    download_web_page(url2)

    with open('new-downloaded-web-page.html', 'r') as f:
        html_source3 = f.read()

    doc3 = BeautifulSoup(html_source3, 'html.parser')

    tr_class_tags3 = doc3.find_all('tr',class_='simpTblRow')

    most_active_stocks_100_200 = [parse_stocks(x) for x in tr_class_tags3]
    return most_active_stocks_100_200

most_active_5_stocks_next_page = parse_stock_pages(100)[:5]
most_active_5_stocks_next_page

most_active_stocks_100_200 = parse_stock_pages(100)

"""### 5. Create a CSV of the parsed information

We can write a function that creates a CSV file from our parsed information.
"""

def write_csv(items, path):
    with open(path, 'w') as f:
        if len(items) == 0:
            return

        headers = list(items[0].keys())
        f.write(','.join(headers) + '\n')

        for item in items:
            values = []
            for header in headers:
                values.append(str(item.get(header, "")))
            f.write(','.join(values) + "\n")

write_csv(most_active_stocks, 'most-active-stocks.csv')

write_csv(most_active_stocks_100_200, 'most-active-stocks-next-page.csv')

with open('most-active-stocks.csv', 'r') as f:
    print(f.read())

with open('most-active-stocks-next-page.csv', 'r') as f:
    print(f.read())

"""### 6. Display the CSV files in Pandas data frames"""

import pandas as pd

most_active_df = pd.read_csv('most-active-stocks.csv')
most_active_df

"""We'll now look at a few different data frames that are sorted by different values."""

order_by_mark_cap_df = most_active_df.sort_values('Market cap', ascending=False)
order_by_mark_cap_df

order_by_pe_ratio_df = most_active_df.sort_values('PE ratio', ascending=False)
order_by_pe_ratio_df

order_by_vol_df = most_active_df.sort_values('Volume', ascending=False)
order_by_vol_df

order_by_price_df = most_active_df.sort_values('Last price of stock')
order_by_price_df

"""### 7. A bit of simple data visualisation

In order to visualise the data frames from above, it can be helpful to look at a plot like a bar chart. These are all very simple bar charts but are intended to shed light on some of the findings from our articles.
"""

import matplotlib.pyplot as plt

"""A plot that shows stocks from the 100 most active stocks from highest to lowest PE ratio."""

order_by_pe_ratio_df.plot(x ='Stock ticker', y='PE ratio', kind = 'bar',figsize=(20,10))
plt.show()

"""A plot that shows stocks from the 100 most active stocks from highest to lowest volume."""

order_by_vol_df.plot(x ='Stock ticker', y='Volume', kind = 'bar',figsize=(20,10),color='orange')
plt.show()

"""A plot that shows stocks from the 100 most active stocks from cheapest to most expensive."""

order_by_price_df.plot(x ='Stock ticker', y='Last price of stock', kind = 'bar',figsize=(20,10),color='pink')
plt.show()

"""### 8. Summary

In summary, we've downloaded two articles, parsed them for information, created functions to create dictionaries of the parsed information, saved those dictionaries as CSV files, converted those CSV files to pandas data frames, and finally, visualised a few of those data frames.

This project is meant to be informative and general in nature, it is not applicable to anyone's personal situation.

### 9. Future work ideas

In the future, I would love to gather more information on individual stocks such as historical data so that we can look at comparisons of stocks historically. I would also like to delve deeper into data displays and derive useful inferences that can be used for educational purposes. I think this project can be used as a springboard for my goal of shrinking the wealth gap.

### 10. References

* “Cryptocurrency List & Prices, Top Cryptocurrencies.” Yahoo! Finance, Yahoo!, 20 Aug. 2022, https://finance.yahoo.com/cryptocurrencies/.
* “Girls That Invest Podcast & Masterclass.” Girls That Invest, https://girlsthatinvest.com/.
* “Most Active Stocks Today.” Yahoo! Finance, Yahoo!, 20 Aug. 2022, https://finance.yahoo.com/most-active?offset=0&amp;count=100.
"""

!pip install jovian --upgrade --quiet

import jovian

jovian.commit(files=['most-active-stocks-next-page.csv', 'most-active-stocks.csv'])