

https://github.com/user-attachments/assets/8f99cfd9-f037-4acd-a9a3-715d04417a68


[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)([https://colab.research.google.com/drive/1Ib4H27ScR0PXdPGVY6cone8xNxlnfcOQ#scrollTo=wmQKva1JhlFH])
# üìä Analyse des Actions les Plus Actives du March√© Boursier : Une Approche par Web Scraping et Traitement de Donn√©es

# üìà Yahoo Finance Scraper ‚Äî Most Active Stocks

> **De la donn√©e brute √† l'intelligence financi√®re** : un outil automatis√© pour collecter, nettoyer et visualiser les actions les plus actives du march√© am√©ricain.

---

## üìå Description et objectif du projet

Ce projet est un **outil de web scraping enti√®rement automatis√©** qui extrait les donn√©es boursi√®res en temps r√©el depuis [Yahoo! Finance](https://finance.yahoo.com/markets/stocks/most-active/), plus pr√©cis√©ment la page des **"Most Active Stocks"** ‚Äî les 200 actions pr√©sentant le volume d'√©change quotidien le plus √©lev√© sur le march√© am√©ricain.

### üéØ Pourquoi ce projet ?

L'investissement est souvent per√ßu comme r√©serv√© √† une √©lite disposant des bons outils et des bonnes informations. Ce projet a une double ambition :

1. **D√©mocratiser l'acc√®s √† la donn√©e financi√®re** en rendant les informations boursi√®res accessibles √† tous, sans abonnement payant.
2. **Servir de support p√©dagogique** pour apprendre √† collecter, traiter et analyser des donn√©es r√©elles avec Python.

### üìä Quelles donn√©es sont collect√©es ?

Pour chaque action, le scraper extrait automatiquement :

| Champ | Description |
|---|---|
| `Stock ticker` | Symbole boursier (ex: NVDA, AAPL) |
| `Stock name` | Nom complet de l'entreprise |
| `Last price of stock` | Dernier prix cot√© en USD |
| `Stock change` | Variation absolue du jour (ex: -4.19) |
| `Stock percentage change` | Variation en pourcentage (ex: -2.24%) |
| `Volume` | Nombre de titres √©chang√©s dans la journ√©e |
| `Average volume over 3 months` | Volume moyen sur 3 mois (r√©f√©rence) |
| `Market cap` | Capitalisation boursi√®re totale |
| `PE ratio` | Ratio Cours/B√©n√©fice |

---

## üóÇÔ∏è Structure du projet

```
Projet-Yahoo-Finance-Scraping/
‚îÇ
‚îú‚îÄ‚îÄ Projet_Technique_scraping.py   # Script principal automatis√©
‚îú‚îÄ‚îÄ Projet_Technique_scraping.ipynb # Notebook Jupyter (version d√©taill√©e/p√©dagogique)
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt               # D√©pendances Python
‚îú‚îÄ‚îÄ .gitignore                     # Fichiers exclus de Git
‚îî‚îÄ‚îÄ README.md                      # Ce fichier
```

---

## ‚öôÔ∏è Instructions d'installation

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/votre-username/yahoo-finance-scraper.git
cd yahoo-finance-scraper
```

### 2. Cr√©er un environnement virtuel (recommand√©)

```bash
python -m venv venv
source venv/bin/activate        # Linux / Mac
venv\Scripts\activate           # Windows
```

### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

---

## üöÄ Exemples d'utilisation

### Lancer le scraper automatis√©

```bash
python Projet_Technique_scraping.py
```

**R√©sultat attendu dans le terminal :**
```
--- D√©marrage du Scraper Automatis√© ---
‚úÖ Autorisation Robots.txt accord√©e pour : https://finance.yahoo.com/most-active
üì• T√©l√©chargement : https://finance.yahoo.com/most-active?count=100&offset=0...
‚è≥ Pause √©thique de 3 secondes...
üì• T√©l√©chargement : https://finance.yahoo.com/most-active?count=100&offset=100...
‚è≥ Pause √©thique de 3 secondes...
‚úÖ Succ√®s ! 50 actions r√©cup√©r√©es.
üìÅ Donn√©es sauvegard√©es dans : data/yahoo_stocks_2026-02-16.csv
```

### Utiliser le notebook Jupyter (version p√©dagogique)

```bash
jupyter notebook Projet_Technique_scraping.ipynb
```

Le notebook d√©taille chaque √©tape avec des explications, des exemples interm√©diaires et des visualisations.

---

## üß† Explication d√©taill√©e du code

### √âtape 1 ‚Äî Configuration et imports

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time, os, urllib.robotparser
from datetime import datetime

BASE_URL = "https://finance.yahoo.com/most-active"
FILENAME = f"yahoo_stocks_{datetime.now().strftime('%Y-%m-%d')}.csv"
```

Avant toute chose, on importe les biblioth√®ques n√©cessaires et on d√©finit les constantes globales. Le nom du fichier CSV inclut la date du jour pour conserver un historique automatique √† chaque ex√©cution.

**R√©sultat :** Un environnement de travail structur√©, avec des param√®tres centralis√©s faciles √† modifier.

**Importance :** Une bonne configuration en amont √©vite de "coder en dur" des valeurs √©parpill√©es dans le script ‚Äî c'est la base d'un outil r√©utilisable et maintenable.

---

### √âtape 2 ‚Äî Headers √©thiques

```python
HEADERS = {
    'User-Agent': 'Mozilla/5.0 ... (Student Project; Contact: email@gmail.com)',
    'Accept-Language': 'en-US,en;q=0.9',
}
```

Lorsqu'un programme acc√®de √† un site web, le serveur voit la requ√™te. Sans header, la requ√™te ressemble √† celle d'un robot malveillant et peut √™tre bloqu√©e. En d√©clarant un `User-Agent` honn√™te avec ses coordonn√©es, on s'identifie de fa√ßon transparente.

**R√©sultat :** Le serveur Yahoo re√ßoit notre requ√™te et la traite normalement, sans nous bloquer.

**Importance :** C'est √† la fois une bonne pratique technique (√©viter les blocages) et une d√©marche √©thique (honn√™tet√© vis-√†-vis du site).

---

### √âtape 3 ‚Äî V√©rification du fichier robots.txt

```python
def check_robots_txt(url, user_agent):
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(ROBOTS_URL)
    rp.read()
    allowed = rp.can_fetch(user_agent, url)
    if allowed:
        print(f"‚úÖ Autorisation Robots.txt accord√©e pour : {url}")
    return allowed
```

Chaque site web dispose d'un fichier `robots.txt` (ex: `https://finance.yahoo.com/robots.txt`) qui indique quelles pages les robots peuvent ou ne peuvent pas visiter. Le respecter est une obligation l√©gale et √©thique.

**R√©sultat :** `‚úÖ Autorisation Robots.txt accord√©e pour : https://finance.yahoo.com/most-active` ‚Äî la page cible est bien autoris√©e.

**Importance :** Ne pas v√©rifier le `robots.txt` expose √† des poursuites l√©gales et √† l'IP ban. C'est la premi√®re chose √† faire avant tout scraping.

---

### √âtape 4 ‚Äî Fonction de nettoyage `parse_value`

```python
def parse_value(value_str):
    if value_str.endswith('T'):
        return float(value_str[:-1]) * 1_000_000_000_000
    elif value_str.endswith('B'):
        return float(value_str[:-1]) * 1_000_000_000
    elif value_str.endswith('M'):
        return float(value_str[:-1]) * 1_000_000
```

Yahoo Finance affiche les grands nombres avec des abr√©viations : `4.45T` (trillions), `161.88M` (millions). Ces cha√Ænes de caract√®res ne peuvent pas √™tre compar√©es directement. Il faut les convertir en valeurs num√©riques exploitables.

**R√©sultat :** `"4.45T"` ‚Üí `4 450 000 000 000` | `"161.88M"` ‚Üí `161 880 000`

**Importance :** Sans cette conversion, il est impossible de trier les actions par capitalisation ou volume. C'est le c≈ìur du nettoyage des donn√©es.

---

### √âtape 5 ‚Äî T√©l√©chargement de la page avec gestion d'erreurs

```python
def get_page_data(offset=0):
    url = f"{BASE_URL}?count=100&offset={offset}"
    response = requests.get(url, headers=HEADERS, timeout=10)
    response.raise_for_status()  # L√®ve une exception si erreur HTTP
    soup = BeautifulSoup(response.text, 'html.parser')
```

`requests.get()` envoie une requ√™te HTTP GET au serveur. Le param√®tre `offset` permet la **pagination** : Yahoo n'affiche que 100 r√©sultats par page, donc `offset=0` donne les rangs 1-100 et `offset=100` donne les rangs 101-200. `raise_for_status()` stoppe proprement le script en cas d'erreur (404, 500, etc.) plut√¥t que de continuer avec des donn√©es vides.

**R√©sultat :** Le contenu HTML de la page est t√©l√©charg√©. Un code de statut `200` confirme le succ√®s.

**Importance :** La gestion d'erreurs rend l'outil robuste. Sans elle, le script peut planter silencieusement et produire des donn√©es incompl√®tes sans que l'utilisateur s'en rende compte.

---

### √âtape 6 ‚Äî Parsing HTML avec BeautifulSoup

```python
table = soup.find('table')
rows = table.find('tbody').find_all('tr')
```

BeautifulSoup "parse" (analyse) le HTML brut pour le rendre navigable comme un arbre. On cherche d'abord la balise `<table>` (le tableau de donn√©es principal), puis toutes les lignes `<tr>` dans son corps `<tbody>`. Chaque ligne `<tr>` correspond √† une action.

**R√©sultat :** Une liste de 25 √† 100 objets `<tr>`, chacun contenant toutes les informations d'une action.

**Importance :** C'est l'√©tape centrale du scraping. Sans parsing, le HTML est une cha√Æne de texte illisible de plusieurs millions de caract√®res.

---

### √âtape 7 ‚Äî Extraction structur√©e avec `parse_stocks`

```python
def parse_stocks(tr_class_tag):
    td_tag = tr_class_tag.find_all('td')
    ticker_name = td_tag[0].find('a', attrs={'data-testid': 'table-cell-ticker'}).text.strip()
    price_span = td_tag[3].find('span', attrs={'data-testid': 'change'})
    # ... etc.
    return {'Stock ticker': ticker_name, 'Last price of stock': price_tag, ...}
```

Chaque cellule `<td>` d'une ligne contient une information sp√©cifique. On utilise les attributs `data-testid` (plus stables que les classes CSS qui changent fr√©quemment) pour cibler pr√©cis√©ment chaque donn√©e.

**R√©sultat :**
```python
{
  'Stock ticker': 'NVDA',
  'Stock name': 'NVIDIA Corporation',
  'Last price of stock': 182.78,
  'Volume': 161888000,
  'Market cap': 4450000000000,
  'PE ratio': 45.25
}
```

**Importance :** Transformer du HTML non structur√© en dictionnaire Python propre est l'objectif final du scraping. Ce dictionnaire peut ensuite √™tre analys√©, stock√© ou export√© facilement.

---

### √âtape 8 ‚Äî Boucle de pagination automatis√©e

```python
for offset in [0, 100]:
    data = get_page_data(offset)
    all_stocks.extend(data)
    time.sleep(3)  # Pause √©thique
```

Yahoo affiche les donn√©es par pages de 100. Pour obtenir les 200 actions, il faut faire deux requ√™tes avec des offsets diff√©rents. Le `time.sleep(3)` est une **pause √©thique** obligatoire entre les requ√™tes.

**R√©sultat :** Une liste consolid√©e de 200 actions (ou moins selon ce que Yahoo retourne).

**Importance :** Sans pagination, on ne r√©cup√®re qu'une partie des donn√©es. Sans la pause, on risque de surcharger le serveur et de se faire bloquer ‚Äî c'est contraire aux bonnes pratiques et potentiellement ill√©gal.

---

### √âtape 9 ‚Äî Sauvegarde en CSV et analyse Pandas

```python
df = pd.DataFrame(all_stocks)
df.to_csv(filepath, index=False)

# Tris pour analyse
order_by_market_cap = df.sort_values('Market cap', ascending=False)
order_by_volume = df.sort_values('Volume', ascending=False)
order_by_price = df.sort_values('Last price of stock')
order_by_pe = df.sort_values('PE ratio', ascending=False)
```

Le CSV offre une persistance des donn√©es : les r√©sultats sont disponibles apr√®s l'ex√©cution, avec un nom horodat√© pour conserver l'historique. Les DataFrames Pandas permettent ensuite d'analyser les donn√©es selon diff√©rents crit√®res d'investissement.

**R√©sultat :** Un fichier `data/yahoo_stocks_2026-02-XX.csv` contenant toutes les donn√©es, et plusieurs vues tri√©es.

**Importance :** C'est la livraison finale de l'outil. Un investisseur peut instantan√©ment identifier les meilleures opportunit√©s selon son profil : cherche-t-il les grandes capitalisations stables (NVDA, AAPL) ? Les penny stocks accessibles (PLUG √† 1.89$) ? Les actions sous-√©valu√©es (PE ratio bas) ?

---

### √âtape 10 ‚Äî Visualisations Matplotlib

```python
order_by_vol_df.plot(x='Stock ticker', y='Volume', kind='bar', figsize=(20,10), color='orange')
plt.show()
```

Un graphique en barres permet de visualiser imm√©diatement les disparit√©s entre actions ‚Äî ce qu'un tableau de chiffres ne rend pas aussi clairement.

**R√©sultats obtenus :**
- **Graphique Volume (orange)** ‚Üí NVDA domine massivement avec 161M de titres √©chang√©s, loin devant RIVN (127M) et RIG (98M). Quelques g√©ants captent la majorit√© de l'attention du march√©.
- **Graphique PE Ratio (bleu)** ‚Üí TSLA (386) et PLTR (208) ont des ratios extr√™mement √©lev√©s, signe d'attentes de croissance importantes ou d'une possible sur√©valuation.
- **Graphique Prix (rose)** ‚Üí Disparit√© totale entre PLUG (1.89$ ‚Äî penny stock) et TSLA (417$). Le prix seul ne refl√®te pas la taille de l'entreprise.

**Importance :** La visualisation transforme des donn√©es brutes en **insights actionnables**. C'est la derni√®re √©tape du pipeline complet : Web ‚Üí Python ‚Üí CSV ‚Üí D√©cision.

---

## üì¶ Liste des d√©pendances

```
requests==2.31.0
beautifulsoup4==4.12.2
pandas==2.1.0
matplotlib==3.7.2
```

Installez-les via :

```bash
pip install -r requirements.txt
```

---

## ‚öñÔ∏è Mentions l√©gales et √©thique

Ce projet respecte scrupuleusement les r√®gles suivantes :

- ‚úÖ **robots.txt v√©rifi√©** avant chaque session de scraping (`finance.yahoo.com/most-active` est autoris√©)
- ‚úÖ **Pauses entre requ√™tes** : minimum 3 secondes entre chaque appel serveur
- ‚úÖ **Identification transparente** dans le User-Agent (nom + contact)
- ‚úÖ **Aucune surcharge serveur** : maximum 2 requ√™tes par session
- ‚úÖ **Donn√©es publiques uniquement** : toutes les informations sont librement accessibles sur Yahoo Finance
- ‚úÖ **Usage √©ducatif et non commercial**
- ‚ö†Ô∏è Ne jamais stocker d'identifiants dans Git (`.gitignore` configur√© en cons√©quence)
- ‚ö†Ô∏è Ce projet est fourni √† titre informatif uniquement et ne constitue pas un conseil en investissement financier

> **Note :** Yahoo Finance propose une API officielle (`yfinance`) qui peut √™tre pr√©f√©rable pour certains usages. Ce projet utilise le scraping direct √† des fins p√©dagogiques, pour illustrer les techniques de parsing HTML.

---

## üë§ Auteurs : M'Bemba CAMARA , Nenad JOVANOVIC

Projet r√©alis√© dans le cadre du cours de **Technique de Programmation**.
